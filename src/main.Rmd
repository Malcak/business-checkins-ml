---
title: "Taller Business Checkins"
authors: "Manuel Alejandro Castaño Jaramillo - Yahaira Hoyos Barrantes - Sebastian Davila Garcia"
date: "4/12/2020"
output: html_notebook
---

# Introducción
Analizaremos los datos proveidos por la plataforma **Yelp** sobre el número de entradas a restaurantes de Pittsburgh para entrenar un modelo de aprendizaje automático que predice el nivel de carga de trabajo de cualquier restaurante de pittsburgh en un día y hora específicos.

## Importando Librerías y Datos

```{r}
## libraries
# the fastdummies library for the one hot encoding process
library(fastDummies)
# the janitor library to clean the column heading of the dataframes
library(janitor)
# the ggplot2 library for creating plots
library(ggplot2)
# reshape2 to melt the correlation matrix
library(reshape2)


# importing business checkin data from yelp
data = read.csv("../data/business_checkin.csv", header = TRUE, sep = ",")

```

# Descripción de los Datos
El conjunto de datos cuenta con **16800** registros donde encontraremos ciertas características de algunos negocios en cada hora de cada día de la semana, de estos datos tan solo **1512** son registros de negocios con la categoría de restaurante y en las horas en que se encontraban abiertos.

Los datos cuentan con las siguientes variables:

* **rownumber**: El índice del registro en la base de datos.
* **business_id**: El identificador del negocio.
* **city**: La ciudad en la que se encuentra el negocio.
* **stars**: Las estrellas con las que contaba el negocio al momento de hacer la medición.
* **review_count**: El número de reviews con las que contaba el negocio al momento de hacer la medición.
* **is_open**: Si el negocio estaba abierto en el momento que se hizo la medición.
* **category**: La categoría con la cual se identifica el negocio.
* **day_time**: El día de la semana y la hora en que se hizo la medición.
* **week_way**: El día de la semana en que se hizo la medición.
* **hourofday**: La hora en que se hizo la medición.
* **checkins**: La cantidad de entradas al negocio en el momento que se hizo la medición.

```{r}
# filtering the data by Restaurants category and by the city of Pittsburgh
data_restaurants <- subset(data, category=="Restaurants" & is_open==TRUE)
data_restaurants <- subset(data_restaurants, city=="Pittsburgh")

head(data_restaurants)
```
Se revolvieron los registros.
```{r}
# shuffle the dataframe
data_restaurants <- data_restaurants[sample(nrow(data_restaurants)),]
```

## Visualización del Comportamiento de los Datos
### Gráfico de la Frecuencia de los Checkins
```{r}
ggplot(
  data=data_restaurants,
  aes(x=checkins)
) + geom_histogram()
```

### Gráfico del Número de Checkins por cada Día de la Semana
```{r}
ggplot(
  data=data_restaurants,
  aes(x=week_day, y=checkins)
) + geom_bar(stat="identity")
```

### Gráfico del Número de Checkins por cada Hora del Día
```{r}
ggplot(
  data=data_restaurants,
  aes(x=hourofday, y=checkins)
) + geom_bar(stat="identity")
```

## Limpiando las Variables no Deseadas

Se eliminarán la siguientes varibales de dataset.

* **rownumber**: Porque es el índice de la fila y no tiene nada que ver con el problema.
* **business_id**: Porque se busca que el modelo prediga la carga laboral en general de los restaurantes de Pittsburgh y no de cada restaurante de forma específica, además si se buscara esto se contaría con muy pocos datos por cada negocio.
* **city**: Porque la única cidad tratada en el problema es Pittsburgh
* **is_open**: Porque en el único momento enq ue un restaurante debería tener entradas es en el momento que esté abierto, el resto se consideraron como errores en el momento de la medición de los datos.
* **category**: Porque la única categoría tratada en el problema son los restaurantes.
* **day_time**: Porque este es un dato redundante, su información ya la aportan las variables de "**week_day**" y "**hourofday**"

```{r}
data_restaurants$ï..rownumber <- NULL
data_restaurants$day_time <- NULL
data_restaurants$business_id <- NULL
data_restaurants$is_open <- NULL
data_restaurants$city <- NULL
data_restaurants$category <- NULL
head(data_restaurants)
```

## Combirtiendo Variables Categoricas en Variables "Dummy"

Se crean variables ficticias para cada uno de los días de las semanas y se marcan con 1 la que corresponde con a el día del registro.
```{r}
data_restaurants <- dummy_cols(
  data_restaurants, select_columns = c("week_day")
)
data_restaurants$week_day <- NULL

## cleaning the column heading
data_restaurants = clean_names(data_restaurants)
head(data_restaurants)
```

## Correlación de los Datos
```{r}
cormat <- round(cor(data_restaurants), 2 )
melted_cormat <- melt(cormat)

# create a ggheatmap
ggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "red", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                   size = 12, hjust = 1))+
  coord_fixed()

ggheatmap +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.border = element_blank(),
    panel.background = element_blank(),
    axis.ticks = element_blank(),
    legend.justification = c(1, 0),
    legend.position = c(1.5, 0.7),
    legend.direction = "horizontal")+
  guides(fill = guide_colorbar(barwidth = 7, barheight = 1,
                               title.position = "top", title.hjust = 0.5))
```

## Transformando Checkins a Categorías
```{r}
data_restaurants$checkins_cat <- cut(data_restaurants$checkins, breaks = c(0, 5, 10, Inf),
                  labels = c("baja", "media", "alta"),
                  right = FALSE)
                  
library(dplyr)

 
# data_restaurants <- data.frame(data_restaurants %>% select(stars,review_count, hourofday, week_day_mon, week_day_tue, week_day_wed, week_day_thu, week_day_fri, week_day_sat, week_day_sun ,checkins, checkins_cat))

# data_restaurants <- data.frame(data_restaurants %>% select(stars,review_count, hourofday, week_day, checkins, checkins_cat))

# data_restaurants$checkins <- NULL
```

## test
```{r}

library(caret)
featurePlot(x = data_restaurants[, 1:3], 
            y = data_restaurants$checkins_cat, 
            plot = "pairs",
            auto.key = list(columns = 3))

# library(WVPlots)
# PairPlot(data_restaurants, colnames(data_restaurants)[1:4], "Datos restaurantes", group_var = "checkins_cat")
```

## proporción de las clases
```{r}
table(data_restaurants$checkins_cat)
```

## Resampling
```{r}
  set.seed(1000) # puede que cambie por el tiempo
library(tidyverse)
# con oversampling
up_dataset <- upSample(x = data_restaurants[, -ncol(data_restaurants)],
                       y = data_restaurants$checkins_cat)
up_dataset <- rename(up_dataset, c("checkins_cat" = "Class"))
table(up_dataset$checkins_cat)

```




```{r}
#Implementacion de Naïve Bayes
# shuffle del data frame y eliminar checkins
up_dataset <- up_dataset[sample(nrow(up_dataset)),]
up_dataset$checkins <- NULL
```



```{r}
#Sets de entrenamiento y prueba
T_datos_entrenamiento<-nrow(up_dataset)*0.8

# Muestra de datos de entrenamiento: Tomar filas desde la 1 hasta el tamaño de la muestra.
datos_entrenamiento<- up_dataset[1:T_datos_entrenamiento,]

datos_pruebas<-up_dataset[(T_datos_entrenamiento+1):nrow(up_dataset),]
```



```{r}
# se desarrolla el modelo a partir de los datos de entrenamiento
library(naivebayes)
modelo_naivebayes <- naive_bayes(formula = checkins_cat ~ .,  data = datos_entrenamiento)
```


```{r}
# se lleva a cabo la etapa de pruebas para determinar la capacidad de prediccion del modelo
prediccion <- predict(modelo_naivebayes, datos_pruebas)
head(prediccion, 25)

# Metricas de evaluacion
confusion<-confusionMatrix(prediccion, datos_pruebas[["checkins_cat"]])
confusion
confusion[["byClass"]]

library(pROC)
auc(as.numeric(prediccion), as.numeric(datos_pruebas[["checkins_cat"]]))

```





```{r}
install.packages("rpart")
install.packages("rpart.plot")
library(rpart)
library(rpart.plot)

arbol_1 <- rpart(formula = checkins_cat ~ ., data = datos_entrenamiento)

rpart.plot(arbol_1)

prediccion_1 <- predict(arbol_1, newdata = datos_pruebas, type = "class")
confusionMatrix(prediccion_1, datos_pruebas[["checkins_cat"]])
```



